{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from GridWorld import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error(Exception):\n",
    "    \"\"\"Base class for exceptions in this module.\"\"\"\n",
    "    pass\n",
    "\n",
    "class InputError(Error):\n",
    "    \"\"\"Exception raised for errors in the input.\"\"\"\n",
    "\n",
    "    def __init__(self, msg):\n",
    "        self.msg = msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration():\n",
    "    \"\"\"Class of a general discrete agent\"\"\"\n",
    "    \n",
    "    def __init__(self, env, discount_factor=0.99, theta=0.000001):\n",
    "        self.env = env\n",
    "        self.ValueFunction = np.zeros(self.env.nS)\n",
    "        self.discount_factor = discount_factor\n",
    "        self.theta = theta\n",
    "        self.policy = np.ones([self.env.nS, self.env.nA])/self.env.nA\n",
    "\n",
    "    def evaluate_policy(self, policy):\n",
    "        # Start with a random (all 0) value function\n",
    "        V = np.zeros(self.env.nS)\n",
    "        while True:\n",
    "            delta = 0\n",
    "            # For each state, perform a \"full backup\"\n",
    "            for s in range(self.env.nS):\n",
    "                v = 0\n",
    "                # Look at the possible next actions\n",
    "                for a, action_prob in enumerate(policy[s]):\n",
    "                    # For each action, look at the possible next state.                    \n",
    "                    next_state, reward, done, prob = self.env.P[s][a]\n",
    "\n",
    "                    v += action_prob * prob * (reward + self.discount_factor * V[(int)(next_state)])\n",
    "                \n",
    "                # How much our value function changed (across any states)\n",
    "                delta = max(delta, np.abs(v - V[s]))\n",
    "                V[s] = v\n",
    "            \n",
    "            # Stop evaluating once our value function change is below a threshold\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "        return np.array(V)\n",
    "    \n",
    "    def next_step_rewards(self, state, V):\n",
    "        A = np.zeros(self.env.nA)\n",
    "        for i in range(self.env.nA):\n",
    "            next_state, reward, done, prob = self.env.P[state][i]\n",
    "            A[i] += prob * (reward + self.discount_factor*V[(int)(next_state)])\n",
    "        return A\n",
    "    \n",
    "    def update_policy(self, V, policy):\n",
    "        \n",
    "        is_stable = True \n",
    "        \n",
    "        for i in range(self.env.nS):\n",
    "            curr_best = np.argmax(policy[i])\n",
    "\n",
    "            rewards = self.next_step_rewards(i, V)\n",
    "\n",
    "            actual_best = np.argmax(rewards)\n",
    "\n",
    "            if curr_best != actual_best:\n",
    "                is_stable = False\n",
    "                \n",
    "            policy[i] =  np.eye(self.env.nA)[actual_best]\n",
    "\n",
    "        return policy, is_stable\n",
    "    \n",
    "    def update(self):\n",
    "        # Start with a random policy\n",
    "        policy = np.ones([self.env.nS, self.env.nA]) / self.env.nA\n",
    "        while True:\n",
    "            V = self.evaluate_policy(policy)\n",
    "            policy, is_stable = self.update_policy(V, policy)\n",
    "            if is_stable:\n",
    "                self.policy = policy\n",
    "                self.ValueFunction = V\n",
    "                return policy, V\n",
    "            \n",
    "    def get_action(self, state):\n",
    "        if type(state) is tuple and len(state) == 2:\n",
    "            return np.random.choice(np.arange(len(self.policy[state])), p=self.policy[state])\n",
    "        raise InputError(\"Wrong State Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PolicyIteration(GridWorld())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol, V = agent.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[826.02716166, 835.38097235, 844.82926597, 854.37299688,\n",
       "        864.01312911, 873.7506364 , 883.58650235, 893.52172046],\n",
       "       [835.38097235, 844.82926597, 854.37299688, 864.01312911,\n",
       "        873.7506364 , 883.58650235, 893.52172046, 903.5572943 ],\n",
       "       [844.82926597, 854.37299688, 864.01312911, 873.7506364 ,\n",
       "        883.58650235, 893.52172046, 903.5572943 , 913.69423757],\n",
       "       [854.37299688, 864.01312911, 873.7506364 , 883.58650235,\n",
       "        893.52172046, 903.5572943 , 913.69423757, 923.93357419],\n",
       "       [864.01312911, 873.7506364 , 883.58650235, 893.52172046,\n",
       "        903.5572943 , 913.69423757, 923.93357419, 934.27633845],\n",
       "       [873.7506364 , 883.58650235, 893.52172046, 903.5572943 ,\n",
       "        913.69423757, 923.93357419, 934.27633845, 944.72357507],\n",
       "       [883.58650235, 893.52172046, 903.5572943 , 913.69423757,\n",
       "        923.93357419, 934.27633845, 944.72357507, 955.27633932],\n",
       "       [893.52172046, 903.5572943 , 913.69423757, 923.93357419,\n",
       "        934.27633845, 944.72357507, 955.27633932, 944.72357592]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.reshape(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'done': 0, 'invalid_move': -2, 'terminated': 20, 'valid_move': -1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GridWorld().rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
